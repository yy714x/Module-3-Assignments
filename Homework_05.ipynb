{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e6cc80",
   "metadata": {},
   "source": [
    "# **Week 5: Feature Selection and Regularization on the Ames Housing Dataset**\n",
    "\n",
    "In last week’s homework, you built and evaluated linear regression models with increasingly rich feature sets. This week, we take the next critical step: **deciding which features to keep—and how much complexity is too much**.\n",
    "\n",
    "In real-world modeling, adding more predictors does not automatically lead to better models. Extra features can increase variance, reduce interpretability, and hurt generalization. This homework explores two complementary strategies for controlling model complexity:\n",
    "\n",
    "* **Feature selection**, where we explicitly choose a subset of predictors\n",
    "* **Regularization**, where we keep all predictors but constrain their influence\n",
    "\n",
    "Specifically, you will experiment with:\n",
    "\n",
    "* **Forward Selection**\n",
    "* **Backward Selection**\n",
    "* **Ridge Regression**\n",
    "\n",
    "Forward and backward selection approach the same problem from opposite directions—one builds a model up feature by feature, while the other prunes features away—and because both are greedy algorithms, they may arrive at different solutions. In contrast, ridge regression does not discard features at all; instead, it keeps all predictors but shrinks their coefficients toward zero, reducing model complexity in a continuous rather than discrete way.\n",
    "\n",
    "Comparing these methods highlights an important modeling choice: whether to simplify a model by selecting features explicitly or by regularizing their influence, and how these different strategies affect generalization performance.\n",
    "\n",
    "There are **4 problems** with **8 graded answers** worth **6 points each**, and you receive **7 points for completion**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f38fcab-bcb2-4b70-a304-6412d94a36df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports\n",
    "\n",
    "import os\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "from sklearn.model_selection   import train_test_split, cross_val_score,RepeatedKFold\n",
    "from sklearn.linear_model      import LinearRegression,Ridge,Lasso\n",
    "from sklearn.model_selection   import GridSearchCV\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics           import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing     import StandardScaler \n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241380b6-01ad-4a90-91ee-d5c4f53b9218",
   "metadata": {},
   "source": [
    "## **Prelude: Download the Preprocessed Ames Housing Dataset**\n",
    "\n",
    "I have stored the dataset as a zipped directory containing the following four files from an 80%-20% split of the Ames dataset after preprocessing (as done in the last homework):\n",
    "\n",
    "- `X_train.csv`         \n",
    "- `y_train.csv`\n",
    "- `X_test.csv`          \n",
    "- `y_test.csv`\n",
    "\n",
    "**TODO:** Run the following cell to download and extract the dataset into the current working directory of this notebook.  \n",
    "Alternatively, you can manually download the zip file from the provided URL, extract it, and place the files in the same directory as your notebook.\n",
    "\n",
    "> ⚠️ **Important:**  \n",
    "> DO NOT use your own version of these datasets from the last homework. The autograder expects the exact split provided in my files for both training and testing sets. Using any other split will result in incorrect results during grading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd27c153-4225-4556-9515-cb6a917f4221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files downloaded and extracted successfully.\n",
      "Training and testing datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Download the Ames Housing Dataset from Snyder's web site\n",
    "\n",
    "# URL to the zip file\n",
    "zip_url = \"https://www.cs.bu.edu/fac/snyder/cs505/Data/ames_housing.zip\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(zip_url)\n",
    "    response.raise_for_status()  # Raise an error for bad status codes\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zipf:\n",
    "        zipf.extractall(\"Ames_Dataset_HW\")\n",
    "    print(\"Files downloaded and extracted successfully.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error downloading the file: {e}\")\n",
    "\n",
    "\n",
    "DATA_DIR = \"Ames_Dataset_HW\"\n",
    "\n",
    "X_train = pd.read_csv(f\"{DATA_DIR}/X_train.csv\")\n",
    "X_test  = pd.read_csv(f\"{DATA_DIR}/X_test.csv\")\n",
    "\n",
    "y_train = pd.read_csv(f\"{DATA_DIR}/y_train.csv\").squeeze(\"columns\")\n",
    "y_test  = pd.read_csv(f\"{DATA_DIR}/y_test.csv\").squeeze(\"columns\")\n",
    "\n",
    "print(\"Training and testing datasets loaded successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac6da55-0dc7-4509-b2d7-afd475af4060",
   "metadata": {},
   "source": [
    "## **Problem One: Forward Selection with the Ames Dataset**\n",
    "\n",
    "For our first experiment we will apply the forward feature selection algorithm to the dataset. Follow these steps, using the notebook from this week's video lesson as a resource as necessary. \n",
    "\n",
    "**If you have not looked at the video yet, please do it now before continuing.**\n",
    "\n",
    "**Steps to Follow:**\n",
    "1. **Observe** that we have provided the `forward_feature_selection` algorithm from the video notebook for you to use. \n",
    "\n",
    "2. **Copy** from the video notebook into the cell indicated below the code that runs forward feature selection and generates a plot (excluding the part that prints the test MSE--we'll do that at the end of this homework). Or write your own version!\n",
    "\n",
    "3. **Modify** this code to display the **Root Mean Squared Error (RMSE)** instead of MSE.  \n",
    "   - *Remember*: RMSE is the square root of MSE, which provides results in dollars rather than dollars squared, making the metric more interpretable.  \n",
    "   - Both the plot of RMSE vs. the number of selected features and the printout of the Best CV Score should use RMSE.\n",
    "\n",
    "4. **Run** the code on `X_train` and `y_train` (keeping all default settings) to:\n",
    "   - Generate the plot of **Root Mean Squared Error (RMSE)** vs. **Features Added**, and  \n",
    "   - Print the **Best Feature Set** found and the **Best CV RMSE Score**.\n",
    "\n",
    "**Hints:**\n",
    "- The feature names will be more legible if you increase the size of the plot and change the angle and size of the xticks, e.g.,\n",
    "\n",
    "        plt.xticks(range(1, len(selected_features_forward) + 1), selected_features_forward, rotation=60, ha='right', fontsize=6) \n",
    "\n",
    "- You may wish to change the scale of the y-axis to better see the behavior around the minimum point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7227d05b-cc9a-4193-9a5e-703172193095",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Forward Feature Selection\n",
    "\n",
    "def forward_feature_selection(X, y, model, \n",
    "                              scoring='neg_mean_squared_error', \n",
    "                              cv = 5, \n",
    "                              tol=None,               # None = no delta cutoff\n",
    "                                                      # use 0.0 for \"no further improvements\"\n",
    "                                                      # and 1e-4 for \"point of diminishing returns\"                                      \n",
    "                              max_features=None,      # None = use all features\n",
    "                              n_jobs=-1,\n",
    "                              verbose=False\n",
    "                             ):\n",
    "    selected_features = []                            # List to store the order of features selected\n",
    "    remaining_features = list(X.columns)              # Features not yet selected\n",
    "    best_scores = []                                  # List to store the CV score after each feature addition\n",
    "    previous_score = float('inf')                     # Initialize previous score for improvement comparison\n",
    "\n",
    "    # Track the best subset of features and its corresponding score\n",
    "    \n",
    "    best_feature_set = None                           # Best combination of features found so far\n",
    "    best_score = float('inf')                         # Best CV score observed so far\n",
    "    \n",
    "    while remaining_features:\n",
    "        scores = {}                                   # Dictionary to hold CV scores for each candidate feature\n",
    "        for feature in remaining_features:\n",
    "            current_features = selected_features + [feature]\n",
    "            \n",
    "            # Compute the CV score for the current set of features (negated MSE, so lower is better)\n",
    "            cv_score = -cross_val_score(model, X[current_features], y, \n",
    "                                        scoring=scoring, cv=cv, n_jobs=n_jobs\n",
    "                                       ).mean()\n",
    "            scores[feature] = cv_score\n",
    "\n",
    "        # Select the feature that minimizes the CV score\n",
    "        best_feature = min(scores, key=scores.get)\n",
    "        current_score = scores[best_feature]\n",
    "            \n",
    "        # Check if the improvement is significant based on the tolerance (tol)\n",
    "        if tol is not None and previous_score - current_score < tol:\n",
    "            if verbose:\n",
    "                print(\"Stopping early due to minimal improvement.\")\n",
    "            break\n",
    "\n",
    "        # Add the best feature to the selected list and update score trackers\n",
    "        selected_features.append(best_feature)\n",
    "        best_scores.append(current_score)\n",
    "        remaining_features.remove(best_feature)\n",
    "        previous_score = current_score\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nFeatures: {selected_features[-3:]}, CV Score (MSE): {current_score:.4f}\")\n",
    "        \n",
    "        # Update the best subset if the current score is better than the best so far\n",
    "        if current_score < best_score:\n",
    "            best_score = current_score\n",
    "            best_feature_set = selected_features.copy()\n",
    "        \n",
    "        # Check if the maximum number of features has been reached\n",
    "        if max_features is not None and len(selected_features) >= max_features:\n",
    "            break\n",
    "\n",
    "    return (\n",
    "        selected_features,      # List of features in the order they were selected (this will be ALL features if max_features == None\n",
    "        best_scores,            # List of cross-validation scores corresponding to each addition in the previous list\n",
    "        best_feature_set,       # The subset of features that achieved the best CV score.\n",
    "        best_score              # The best CV score\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dca9413-91ff-46c1-9a95-a284bdc4f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:  Run Forward Feature Selection, plot the results, and print out the Best Feature Set and the Best CV Score found. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9572c997-88f7-4373-9e84-8da9350da209",
   "metadata": {},
   "source": [
    "### Problem One Graded Questions\n",
    "\n",
    "Assign `a1a` to the number of features in the best feature set found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4fecff6-613e-4950-b65c-ddbd4d4006d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here; use an expression, not a constant derived by examining the data\n",
    "\n",
    "a1a = 0                     # replace 0 with an expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d01a9f-72b0-46b1-8d76-9fac767cb793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1a = 0\n"
     ]
    }
   ],
   "source": [
    "# Do not change this cell in any way\n",
    "\n",
    "print(f\"a1a = {a1a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f794640-c8fd-4e72-92f3-46d53f63e82f",
   "metadata": {},
   "source": [
    "Assign `a1b` to the best CV RMSE score found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56c91bdb-203f-48b5-bb70-7b755863f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here; use an expression, not a constant derived by examining the data\n",
    "\n",
    "a1b = 0                     # replace 0 with an expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4414dc04-9447-4fa1-8f9a-69bef939964f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1b = $0.00\n"
     ]
    }
   ],
   "source": [
    "# Do not change this cell in any way\n",
    "\n",
    "print(f\"a1b = ${a1b:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb9d5e-a422-4295-94a6-ba9db542ecec",
   "metadata": {},
   "source": [
    "## **Problem Two: Backward Selection with the Ames Dataset**\n",
    "\n",
    "Now, repeat the same process as in Problem One, but using the `backward_feature_selection` algorithm from this week’s video notebook. Again, we will use 5-Fold CV scoring. \n",
    "\n",
    "**Steps to Follow:**\n",
    "1. **Observe** that we have provided the `backward_feature_selection` algorithm from this week's video notebook for you to use.\n",
    "2. **Run** the backward selection algorithm on the Ames dataset (`X_train` and `y_train`).\n",
    "3. **Plot** the results: Display the Root Mean Squared Error (RMSE) vs. the features removed by the algorithm.\n",
    "4. **Print** the Best Feature Set found by backward selection and the corresponding best CV RMSE Score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "413a71be-f510-43fa-8821-4715f3126e44",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Backward Feature Selection\n",
    "\n",
    "def backward_feature_selection(X, y, model, \n",
    "                               scoring='neg_mean_squared_error', \n",
    "                               cv = 5, \n",
    "                               tol=None,               # None = no delta cutoff\n",
    "                                                       # use 0.0 for \"no further improvements\"\n",
    "                                                       # and 1e-4 for \"point of diminishing returns\"                                      \n",
    "                               max_features=None,      # If None, remove features until only 1 remains\n",
    "                                                       # Otherwise, stop when this many features remain\n",
    "                               n_jobs=-1,\n",
    "                               verbose=True\n",
    "                              ):\n",
    "    \n",
    "    # Helper function to compute CV score using LinearRegression\n",
    "    def cv_score(features):\n",
    "        return -cross_val_score(model, X[features], y, \n",
    "                                scoring=scoring, cv=cv, \n",
    "                                n_jobs=n_jobs          ).mean()\n",
    "    \n",
    "    # Start with all features (using a list for easier manipulation)\n",
    "    features_remaining = list(X.columns)\n",
    "    \n",
    "    # Compute initial CV score with the full feature set\n",
    "    initial_score = cv_score(features_remaining)\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    best_score        = initial_score                # Best (lowest) CV score observed so far\n",
    "    best_feature_set  = features_remaining.copy()    # Feature set corresponding to best_score\n",
    "    selected_features = ['NONE']                     # List to record the order in which features are removed\n",
    "    best_scores       = [initial_score]              # List to record the CV score after each removal (starting with full set)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Start with full set of features:\")\n",
    "        print(f'{features_remaining}  CV score (MSE): {np.around(initial_score, 4)}\\n')\n",
    "    \n",
    "    # Determine the target number of features to keep:\n",
    "    # For backward elimination, if max_features is None, we remove until 1 feature remains.\n",
    "    target_feature_count = 1 if max_features is None else max_features\n",
    "    \n",
    "    prev_score = initial_score\n",
    "    round_num = 1\n",
    "    # Continue removing features until we reach the target count\n",
    "    while len(features_remaining) > target_feature_count:\n",
    "        if verbose:\n",
    "            print(f'Round {round_num}:')\n",
    "            \n",
    "        # Initialize variables to track the best removal in this round\n",
    "        lowest_score = float('inf')\n",
    "        feature_to_remove = None\n",
    "        best_new_features = None\n",
    "        \n",
    "        # Try removing each feature one at a time\n",
    "        for feature in features_remaining:\n",
    "            new_feature_set = features_remaining.copy()\n",
    "            new_feature_set.remove(feature)\n",
    "            new_score = cv_score(new_feature_set)\n",
    "            if verbose:\n",
    "                print('Trying removal of:',feature, np.around(new_score, 4))\n",
    "            if new_score < lowest_score:\n",
    "                lowest_score = new_score\n",
    "                feature_to_remove = feature\n",
    "                best_new_features = new_feature_set\n",
    "        \n",
    "        # Check if improvement is significant enough (if tol is set)\n",
    "        if tol is not None and (prev_score - lowest_score) < tol:\n",
    "            if verbose:\n",
    "                print(\"\\nStopping early due to minimal improvement.\")\n",
    "            break\n",
    "        \n",
    "        # Update the best score and feature set if current removal improves performance\n",
    "        if lowest_score < best_score:\n",
    "            best_score = lowest_score\n",
    "            best_feature_set = best_new_features.copy()\n",
    "        \n",
    "        # Update trackers for this round\n",
    "        prev_score = lowest_score\n",
    "        features_remaining = best_new_features\n",
    "        selected_features.append(feature_to_remove)\n",
    "        best_scores.append(lowest_score)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'\\nRemoving {feature_to_remove}:  CV score (MSE) {np.around(lowest_score, 4)}\\n')\n",
    "        round_num += 1\n",
    "\n",
    "    return (\n",
    "        selected_features,      # Order in which features were removed\n",
    "        best_scores,            # CV scores after each removal step\n",
    "        best_feature_set,       # Feature set that achieved the best CV score\n",
    "        best_score              # Best (lowest) CV score\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6af2391-6f2a-4255-9eca-82e98fc3211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:  Run Backward Feature Selection, plot the results, and print out the Best Feature Set and the Best CV RMSE Score found. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98687287-29bd-41f2-9323-f90d7ea3a49a",
   "metadata": {},
   "source": [
    "### Problem Two Graded Questions\n",
    "\n",
    "Assign `a2a` to the number of features in the best feature set found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81158187-be32-4a27-ad62-107f9550210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here; use an expression, not a constant derived by examining the data\n",
    "\n",
    "a2a = 0                     # replace 0 with an expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "988d0fb1-6837-470e-b835-859a09a763f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2a = 0\n"
     ]
    }
   ],
   "source": [
    "# Do not change this cell in any way\n",
    "\n",
    "print(f\"a2a = {a2a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00238e72-993a-4a96-9cf9-c9eb756dbcc6",
   "metadata": {},
   "source": [
    "Assign `a2b` to the best CV RMSE score found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "256564af-ee5b-466f-946d-c8507c8cba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here; use an expression, not a constant derived by examining the data\n",
    "\n",
    "a2b = 0                     # replace 0 with an expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b05cff78-19ec-4ddd-bbc6-379e500a468a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2b = $0.00\n"
     ]
    }
   ],
   "source": [
    "# Do not change this cell in any way\n",
    "\n",
    "print(f\"a2b = ${a2b:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e62d6-b553-4dc8-bb7b-5439a260c5ce",
   "metadata": {},
   "source": [
    "## **Problem Three: Ridge Regression on the Ames Housing Dataset**\n",
    "\n",
    "In this problem, we will apply Ridge Regression to the Ames Housing dataset. Ridge Regression includes a hyperparameter $\\alpha$ that controls the strength of the regularization penalty, which helps prevent overfitting by constraining the growth of the model’s coefficients. A higher $\\alpha$ penalizes large coefficients more, while a lower $\\alpha$ allows them to grow larger.\n",
    "\n",
    "When creating the model, the parameter must be specified, for example:\n",
    "\n",
    "```python\n",
    "ridge_model = Ridge(alpha=100)\n",
    "```\n",
    "\n",
    "This introduces another instance of the model selection problem: we must determine the value of $\\alpha$ that yields the best CV RMSE score.\n",
    "\n",
    "**Steps to Follow:**\n",
    "\n",
    "1. **Standardize the Data:**  \n",
    "   Ridge regression is sensitive to the scale of the features, so we will use `StandardScaler` to standardize the feature set to have a mean of 0 and a standard deviation of 1 before training and testing. Note that the target variable does **not** need to be scaled.\n",
    "\n",
    "2. **Perform Cross-Validation Over a Range of Alpha Values:**  \n",
    "   For each $\\alpha \\in \\{100, 110, 120, \\dots, 500\\}$, calculate the cross-validation RMSE score for the model using 5-Fold CV scoring.  \n",
    "\n",
    "3. **Visualize the Results:**  \n",
    "   Plot the CV RMSE scores against the $\\alpha$ values.\n",
    "\n",
    "4. **Identify the Best Alpha:**  \n",
    "   Determine the $\\alpha$ that results in the minimum CV RMSE Score and print it out, along with the score.\n",
    "\n",
    "**Tip:** It would be an *excellent* idea to add the suffix `_scaled` to any set to which you apply scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bdd7829-c8b5-4911-a739-e5e61b227b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb07668d-47c4-4928-bf7f-cf4ff0d6804f",
   "metadata": {},
   "source": [
    "### Problem Three Graded Answers\n",
    "\n",
    "Set `a3a` to the alpha determined to have the minimum CV RMSE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a77a0908-06f0-414e-953f-19a541c1b8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here; use an expression, not a constant derived by examining the data\n",
    "\n",
    "a3a = 0                     # replace 0 with an expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3a1af1c-3812-4bef-bb96-a2a258bc6999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3a = 0\n"
     ]
    }
   ],
   "source": [
    "# Do not change this cell in any way. \n",
    "\n",
    "print(f\"a3a = {a3a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f57e021-0e48-4d63-9c5e-f0d0b65078d4",
   "metadata": {},
   "source": [
    "Set `a3b` to the CV score found at that alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5cacf50-2f40-4c1f-ab57-54cbc8f39fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here; use an expression, not a constant derived by examining the data\n",
    "\n",
    "a3b = 0                     # replace 0 with an expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26c1e982-1e88-40fb-a896-6206b79fdd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3b = $0.00\n"
     ]
    }
   ],
   "source": [
    "# Do not change this cell in any way. \n",
    "\n",
    "print(f\"a3b = ${a3b:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb57239-029f-4f53-8d8f-7ad5006a7b7e",
   "metadata": {},
   "source": [
    "## **Problem Four: Evaluate Your Best Model**\n",
    "\n",
    "In this final problem, you will identify the model with the best cross-validation RMSE score and evaluate its performance on the held-out test set.\n",
    "\n",
    "#### **Steps to Follow:**\n",
    "\n",
    "1. **Identify the Best Model:**\n",
    "   From your previous results, select the model that achieved the lowest CV RMSE score. \n",
    "\n",
    "3. **Train and Test the Selected Model:** \n",
    "   - For Forward or Backward Feature Selection:  Train a Linear Regression model using `X_train` **restricted to the selected best feature set**, and evaluate it on `X_test` restricted to the same feature set.\n",
    "   - For Ridge Regression:   Train a Ridge Regression model using the best alpha found from cross-validation, with `StandardScaler` applied to both `X_train` and `X_test` before training and testing.\n",
    "\n",
    "4. **Report Your Results:**\n",
    "   Print the name of the best model and the test RMSE in dollars.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bea61bba-4433-4928-9a5b-8dd281ca1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f254631-5f4d-4b2a-b495-8c9865369f1c",
   "metadata": {},
   "source": [
    "### Problem Four Graded Answers\n",
    "\n",
    "Set `a4a` to the best model, according to the CV RMSE score.\n",
    "\n",
    "- 1 = Forward Selection\n",
    "- 2 = Backward Selection\n",
    "- 3 = Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60e8b581-3764-4c02-b3ac-1a883e16d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "\n",
    "a4a = 0                     # replace 0 with one of [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "395043f6-84ad-4d07-a029-160b29537131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a4a = 0\n"
     ]
    }
   ],
   "source": [
    "# Do not change this cell in any way. \n",
    "\n",
    "print(f\"a4a = {a4a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68877104-7f7a-41c1-89c8-173288d268e0",
   "metadata": {},
   "source": [
    "Set `a4b` to the test RMSE for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afe97ea9-9370-49aa-a99a-f08cfa433376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here; use an expression, not a constant derived by examining the data\n",
    "\n",
    "a4b = 0                     # replace 0 with an expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "371f2f02-b7d3-462e-9ed7-90a0c98c87d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a4b = $0.00\n"
     ]
    }
   ],
   "source": [
    "# Do not change this cell in any way. \n",
    "\n",
    "print(f\"a4b = ${a4b:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e8a08b-fdaa-4aaf-9448-a2064374ee3e",
   "metadata": {},
   "source": [
    "### Appendix: Final \"Pause and Ponder\" issue (ungraded) ....\n",
    "\n",
    "Why didn't we evaluate *all* the models on the test set and compare them?\n",
    "\n",
    "### Some answers...\n",
    "\n",
    "It’s tempting to reconsider your choice based on test scores, but this violates the core principle of not \"training to the test.\" That’s why we deliberately avoided evaluating all models on the test set.\n",
    "\n",
    "The test set doesn’t influence model training; it serves as a final, unbiased check on how well your chosen model generalizes to unseen data, ensuring that your selection process wasn’t overly optimistic.\n",
    "\n",
    "A natural **next question** is: Why use a test set at all? Beyond being an industry-standard practice that simulates the real world—where you can’t adjust your model after deployment—consider:\n",
    "\n",
    "- **Validating your workflow:** A strong test score confirms that your model selection process (feature selection, hyperparameter tuning, etc.) was sound. A poor test score despite good cross-validation results may indicate overfitting or a flawed approach.\n",
    "  \n",
    "- **Guarding against over-optimism:** Cross-validation can be slightly optimistic, especially with limited data or repeated trials. The test set provides a final safeguard against relying on an overly tuned model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
